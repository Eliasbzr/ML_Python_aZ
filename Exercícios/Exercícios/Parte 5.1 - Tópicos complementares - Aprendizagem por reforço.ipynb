{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOU0XKPg2jScXk+QgJd4S/r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import random\n","import os\n","\n","from time import sleep\n","\n","import gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from IPython.display import clear_output"],"metadata":{"id":"UOe_mtHm2Zf1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Esta aula foi desenvolvida com esta versão da biblioteca `gym`:"],"metadata":{"id":"HatZ11DYDM9f"}},{"cell_type":"code","source":["gym.__version__"],"metadata":{"id":"oWOnzYFF-t9R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Aprendizagem por reforço com Q-Learning"],"metadata":{"id":"ZIOxIvLpDU1z"}},{"cell_type":"markdown","source":["Neste exercícios, você vai utilizar o ambiente [CliffWalking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking)."],"metadata":{"id":"SwnzT9gLDfZE"}},{"cell_type":"code","source":["env = gym.make(\"CliffWalking-v0\")"],"metadata":{"id":"SC8XDb4c3SBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Resete o ambiente."],"metadata":{"id":"PItvhMjTEAOj"}},{"cell_type":"code","source":["env.___"],"metadata":{"id":"XHAy1qaj3_pj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Este ambiente prevê que o personagem sempre retorna à célula 36 quando o ambiente é resetado."],"metadata":{"id":"M8_ZV0euEDmm"}},{"cell_type":"markdown","source":["Renderize o ambiente, passando o argumento `mode=\"rgb_array\"`. Para visualizar a figura, a chamada para `render` deve ser passada para a função `plt.imshow`."],"metadata":{"id":"oCJvM-E3EKdl"}},{"cell_type":"code","source":["plt.imshow(env.render(mode=___))"],"metadata":{"id":"fRdMbxSbAcnY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exiba o espaço de ações."],"metadata":{"id":"fgHlYKOXEdWy"}},{"cell_type":"code","source":["env.___"],"metadata":{"id":"vHDJa-MF4Ps1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["De acordo com a documentação:\n","\n","> Existem 4 ações discretas determinísticas:\n",">\n","> 0: mover para cima\n",">\n","> 1: mover para a direita\n",">\n","> 2: mover para baixo\n",">\n","> 3: mover para a esquerda"],"metadata":{"id":"WOKtggvmEklm"}},{"cell_type":"markdown","source":["Exiba o espaço de observações."],"metadata":{"id":"OQdSHH3mE1PN"}},{"cell_type":"code","source":["___"],"metadata":{"id":"wHrOaK-y6Q8Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["De acordo com a documentação:\n","\n","> O tabuleiro é uma matriz 4x12.\n","\n","Isso fica evidente pela figura."],"metadata":{"id":"-Nj_3xAtE5gd"}},{"cell_type":"markdown","source":["Exiba os estados possíveis junto com suas ações, probabilidades, novos estados, recompensas e status."],"metadata":{"id":"fw9dZQTNF8Dv"}},{"cell_type":"code","source":["___"],"metadata":{"id":"2SIdJb627Mgm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Observe que as recompensas são iguais a $-1$ ou $-100$. De acordo com a documentação:\n","\n","> Cada passo resulta em recompensa $-1$, e um passo no abismo resulta em recompensa $-100$."],"metadata":{"id":"23FhRRMwGkTh"}},{"cell_type":"markdown","source":["Inicialize a tabela Q para armazenar a memória do treinamento, e exiba seu `shape`."],"metadata":{"id":"NIlSJnQSG1SB"}},{"cell_type":"code","source":["q_table = np.___([___, ___])\n","q_table.___"],"metadata":{"id":"zV3xbHuc6W1-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Agora, pode implementar o algoritmo Q-Learning, utilizando os mesmos parâmetros `alpha`, `gamma` e `epsilon` apresentados em aula. Como este ambiente é mais simples, pode repetir o treinamento por apenas 1000 episódios. Lembre-se de adequar o valor da recompensa na parte do script que contabiliza uma penalidade."],"metadata":{"id":"1uXqoXroG78_"}},{"cell_type":"code","source":["alpha = ___\n","gamma = ___\n","epsilon = ___\n","\n","for i in range(___):\n","    ___\n","\n","print('Treinamento concluído')"],"metadata":{"id":"26jLOWuV7WBA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Resete e renderize o ambiente de novo."],"metadata":{"id":"aKZvssYhHS7-"}},{"cell_type":"code","source":["env.___\n","plt.imshow(env.re___(___))"],"metadata":{"id":"emn240Q78Uup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Agora dê um passo para a direita e renderize o ambiente."],"metadata":{"id":"-n-ZykirHWUR"}},{"cell_type":"code","source":["env.st___(___)\n","plt.imshow(___)"],"metadata":{"id":"aPBTOBbX8WbC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como na direita existe o precipício, este ambiente foi configurado para que, neste caso, o personagem não saia do lugar (mas a recompensa recebida é $-100$)."],"metadata":{"id":"xvhyd-AOKSMN"}},{"cell_type":"markdown","source":["Repita com um passo para cima."],"metadata":{"id":"16FK4TjUHa8d"}},{"cell_type":"code","source":["env.___\n","___"],"metadata":{"id":"YIX_mJ5e8yR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Agora sim o personagem se moveu."],"metadata":{"id":"MwY0alWEJ_tM"}},{"cell_type":"markdown","source":["Para verificar o resultado do treinamento, simule os passos no ambiente, fazendo uso da `q_table`. Como o personagem inicia sempre da mesma célula, não é necessário simular mais do que 1 episódio."],"metadata":{"id":"3FNPcQF5Hd91"}},{"cell_type":"code","source":["total_penalties = ___\n","episodes = ___\n","frames = ___\n","\n","for _ in range(episodes):\n","    ___\n","\n","print('Episódios', episodes)\n","print('Penalidades', total_penalties)"],"metadata":{"id":"T5NOny4m80KM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exiba os frames, admitindo 1 segundo de intervalo entre as exibições."],"metadata":{"id":"M7x4_m35IErS"}},{"cell_type":"code","source":["for frame in frames:\n","    clear_output(wait=True)\n","    plt.imshow(frame['frame'])\n","    plt.show()\n","    print('State', frame['state'])\n","    print('Action', frame['action'])\n","    print('Reward', frame['reward'])\n","    sleep(1)"],"metadata":{"id":"CGHQ8RHc9Ufa"},"execution_count":null,"outputs":[]}]}